# python main.py --config-path="dataset_configs/arabic/youtube/" --config-name="config_whisper.yaml"
processors_to_run: "1:"

split: train # specify dataset type (clean_train, clean_test, ...)
model_name: whisper-v3
dataset_name: quran_dev
dataset_dir: /home/lgrigoryan/prog/asr_commonvoice_finetuning/NeMo-speech-data-processor/workdir/wer/datasets/${dataset_name}
workspace_dir: /home/lgrigoryan/prog/asr_commonvoice_finetuning/NeMo-speech-data-processor/workdir/whisper/${dataset_name}
results_dir: /home/lgrigoryan/prog/asr_commonvoice_finetuning/NeMo-speech-data-processor/workdir/wer/results/${model_name}

processors:
  # 1 getting whisper predictions
  - _target_: sdp.processors.ASRTransformers #pip install accelerate transformers
    pretrained_model: "openai/whisper-large-v3" #"openai/whisper-large-v3"
    batch_size: 16
    device: cuda:0
    output_text_key: pred_text
    input_manifest_file: ${dataset_dir}/manifest_absolute.json
    output_manifest_file: ${workspace_dir}/manifest_predictions_${dataset_name}.json

  # 1 dropping non alphabetical symbols
  - _target_: sdp.processors.DropNonAlphabet
    text_key: pred_text 
    alphabet: " \u0631\u0630\u062F\u062E\u062D\u062C\u062B\u062A\u0629\u0628\u0627\u0626\u0625\u0624\u0623\u0622\u0621\u064A\u0649\u0648\u0647\u0646\u0645\u0644\u0643\u0642\u0641\u063A\u0639\u0638\u0637\u0636\u0635\u0634\u0633\u0632\u064B\u064C\u064D\u064E\u064F\u0650\u0651\u0652"
    input_manifest_file: ${workspace_dir}/manifest_predictions_${dataset_name}.json
    output_manifest_file: ${workspace_dir}/manifest_predictions_processed_${dataset_name}.json

   # 2 canonically decomposing then composing reference text
  - _target_: sdp.processors.ArabicTextPreprocessor
    input_text_key: text
    output_text_key: text
    apply_canonical_decomposition_canonical_composition: True
    output_manifest_file: ${results_dir}/${dataset_name}/manifest_temp.json

  # 3 canonically decomposing then composing predicted text
  - _target_: sdp.processors.ArabicTextPreprocessor
    input_text_key: pred_text
    output_text_key: pred_text
    apply_canonical_decomposition_canonical_composition: True
    output_manifest_file: ${results_dir}/${dataset_name}/manifest_${dataset_name}.json

  # 4 removing diacritics from source text
  - _target_: sdp.processors.ArabicTextPreprocessor
    input_text_key: text
    output_text_key: text
    remove_diacritics: True
    apply_canonical_decomposition_canonical_composition: True
    input_manifest_file: ${workspace_dir}/manifest_predictions_processed_${dataset_name}.json
    output_manifest_file: ${results_dir}/${dataset_name}/manifest_temp.json

  # 5 removing diacritics from predicted text
  - _target_: sdp.processors.ArabicTextPreprocessor
    input_text_key: pred_text
    output_text_key: pred_text
    remove_diacritics: True
    apply_canonical_decomposition_canonical_composition: True
    output_manifest_file: ${results_dir}/${dataset_name}/manifest_${dataset_name}_rm_diacr.json

  # 6 normalizing source text
  - _target_: sdp.processors.ArabicTextPreprocessor
    input_text_key: text
    output_text_key: text
    normalize: True
    apply_canonical_decomposition_canonical_composition: True
    input_manifest_file: ${workspace_dir}/manifest_predictions_processed_${dataset_name}.json
    output_manifest_file: ${results_dir}/${dataset_name}/manifest_temp.json

  # 7 normalizing predicted text
  - _target_: sdp.processors.ArabicTextPreprocessor
    input_text_key: pred_text
    output_text_key: pred_text
    normalize: True
    apply_canonical_decomposition_canonical_composition: True
    output_manifest_file: ${results_dir}/${dataset_name}/manifest_${dataset_name}_normalize.json
